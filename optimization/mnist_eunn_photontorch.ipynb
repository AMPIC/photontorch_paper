{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lCT5P4MY1qPd"
   },
   "source": [
    "# Photontorch MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdw58MPr1qPg"
   },
   "source": [
    "A custom recurrent neural network based on photonic components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39fba1ef-ec4b-38ca-fca0-0bdb31e294f4",
    "colab_type": "text",
    "id": "lyF1rTE41qPi"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "6efa16c1-f457-45bb-49a3-69e11c7fae18",
    "colab": {},
    "colab_type": "code",
    "id": "ohECfupL1qPl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/flaport/Python/Photontorch/photontorch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# standard library\n",
    "import re\n",
    "import os\n",
    "import gzip\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# photontorch\n",
    "import torch\n",
    "import photontorch\n",
    "from photontorch import Component, Network, ClementsNxN, Environment, Mmi, Source, Detector, set_environment\n",
    "print(photontorch.__file__)\n",
    "\n",
    "# other\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set seeds\n",
    "torch.manual_seed(5)\n",
    "np.random.seed(1)\n",
    "\n",
    "# folder to save checkpoints:\n",
    "CHECKPOINTFOLDER = 'mnist_checkpoints/eunn_photontorch_capacity2'\n",
    "set_environment(frequency_domain=True, enable_grad=True)\n",
    "\n",
    "# mpl style context manager\n",
    "custom_style = lambda : plt.style.context(os.path.abspath('custom.mplstyle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WuwFT-U85z5a"
   },
   "source": [
    "## Fetch MNIST Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Adu2Ze1Q529l"
   },
   "outputs": [],
   "source": [
    "def fetch_mnist(redownload=False, verbose=True):\n",
    "    ''' Get MNIST data in npy format\n",
    "\n",
    "    Args:\n",
    "        redownload=False (bool): force redownload, even if file already exists\n",
    "    '''\n",
    "    # check if data is already downloaded. If so, do not download again, except\n",
    "    # when explicitly asked to do so\n",
    "    if (os.path.exists('mnist_data/train.npy')\n",
    "        and os.path.exists('mnist_data/test.npy')\n",
    "        and not redownload):\n",
    "        # load files from data folder\n",
    "        return np.load('mnist_data/train.npy'), np.load('mnist_data/test.npy')\n",
    "\n",
    "    # create folders\n",
    "    if not os.path.isdir('mnist_data'):\n",
    "        os.mkdir('mnist_data')\n",
    "\n",
    "    # check if data is already downloaded. If so, do not download again, except\n",
    "    # when explicitly asked to do so\n",
    "    if not (os.path.exists('mnist_data/train_images.gz')\n",
    "        and os.path.exists('mnist_data/train_labels.gz')\n",
    "        and os.path.exists('mnist_data/test_images.gz')\n",
    "        and os.path.exists('mnist_data/test_labels.gz')\n",
    "        and not redownload):\n",
    "        if verbose:\n",
    "            print('downloading mnist data from http://yann.lecun.com/')\n",
    "        # download data\n",
    "        urlretrieve('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', 'mnist_data/train_images.gz')\n",
    "        urlretrieve('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', 'mnist_data/train_labels.gz')\n",
    "        urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', 'mnist_data/test_images.gz')\n",
    "        urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', 'mnist_data/test_labels.gz')\n",
    "\n",
    "    # fill numpy arrays:\n",
    "    train = np.empty((60000,785), dtype='uint8')\n",
    "    test = np.empty((10000,785), dtype='uint8')\n",
    "    \n",
    "    if verbose:\n",
    "        print('converting .gz data to .npy')\n",
    "\n",
    "    for type, npdata in [('train', train),('test', test)]:\n",
    "        # open the files\n",
    "        with gzip.open('mnist_data/%s_images.gz'%type, 'rb') as data,\\\n",
    "             gzip.open('mnist_data/%s_labels.gz'%type, 'rb') as labels:\n",
    "\n",
    "            # skip the first bytes with metadata of the ubyte file:\n",
    "            data.read(16)\n",
    "            labels.read(8)\n",
    "\n",
    "            # read each byte of the gzip file and save it as a uint8 number\n",
    "            # in the numpy array.\n",
    "            for i in range(npdata.shape[0]):\n",
    "                npdata[i,0] = ord(labels.read(1))\n",
    "                for j in range(784): # append the data after the label\n",
    "                    npdata[i, j+1] = ord(data.read(1))\n",
    "\n",
    "    # save numpy arrays\n",
    "    np.save('mnist_data/train.npy', train)\n",
    "    np.save('mnist_data/test.npy', test)\n",
    "    \n",
    "    if verbose:\n",
    "        print('finished conversion.')\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-QiR5QtN6Pat"
   },
   "source": [
    "## EUNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0Qu3vZo1qPs"
   },
   "outputs": [],
   "source": [
    "class EUNN(ClementsNxN):\n",
    "    def __init__(self, hidden_size, capacity=None):\n",
    "        \"\"\" EUNN __init__\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): the size of the unitary matrix this cell represents.\n",
    "            capacity (int): 0 < capacity <= hidden_size. This number represents the\n",
    "                number of layers containing unitary rotations. The higher the capacity,\n",
    "                the more of the unitary matrix space can be filled. This obviously\n",
    "                introduces a speed penalty. In recurrent neural networks, a small\n",
    "                capacity is usually preferred.\n",
    "        \"\"\"\n",
    "        self.hidden_size = int(hidden_size)\n",
    "        self.capacity = int(hidden_size) if capacity is None else int(capacity)\n",
    "        super(EUNN, self).__init__(N=self.hidden_size, capacity=self.capacity)\n",
    "    \n",
    "    def terminate(self):\n",
    "        ret = super(EUNN, self).terminate()\n",
    "        ret.__class__ = self.__class__ # little hack to keep custom forward function\n",
    "        return ret\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ''' x.shape = (#batches, #features, 2=(real_imag)) \n",
    "        \n",
    "        Note:\n",
    "            this forward method is only valid if no delays are present\n",
    "            in the EUNN\n",
    "        '''\n",
    "        assert torch.is_tensor(x)\n",
    "        num_batches, num_features, _ = x.shape\n",
    "        x = torch.cat([x, torch.zeros((num_batches, self.nmc-num_features, 2), device=self.device)], 1)\n",
    "        rx, ix = x.permute(2,1,0)[:,None,:,:] # rx.shape = (1, #mc nodes, #batches)\n",
    "        rx, ix = self._rC.bmm(rx) - self._iC.bmm(ix), self._rC.bmm(ix) + self._iC.bmm(rx)\n",
    "        det = torch.stack([\n",
    "            rx[0, -self.num_detectors:, :],\n",
    "            ix[0, -self.num_detectors:, :],\n",
    "        ], 0) # det.shape = (2, # detectors, # batches)\n",
    "        return det.permute(2,1,0)\n",
    "    \n",
    "class ModReLU(torch.nn.Module):\n",
    "    \"\"\" A modular ReLU activation function for complex-valued tensors \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        super(ModReLU, self).__init__()\n",
    "        self.bias = torch.nn.Parameter(torch.rand(1, size))\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x, eps=1e-5):\n",
    "        \"\"\" ModReLU forward\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): A torch float tensor with the real and imaginary part\n",
    "                stored in the last dimension of the tensor; i.e. x.shape = [a, ...,b, 2]\n",
    "        Kwargs:\n",
    "            eps (float): A small number added to the norm of the complex tensor for\n",
    "                numerical stability.\n",
    "        \"\"\"\n",
    "        x_re, x_im = x[..., 0], x[..., 1]\n",
    "        norm = torch.sqrt(x_re ** 2 + x_im ** 2) + 1e-5\n",
    "        phase_re, phase_im = x_re / norm, x_im / norm\n",
    "        activated_norm = self.relu(norm + self.bias)\n",
    "        modrelu = torch.stack(\n",
    "            [activated_norm * phase_re, activated_norm * phase_im], -1\n",
    "        )\n",
    "        return modrelu\n",
    "\n",
    "\n",
    "class EURNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, capacity=2):\n",
    "        super(EURNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = int(hidden_size)\n",
    "        self.input_layer = torch.nn.Linear(input_size, hidden_size, bias=True)\n",
    "        self.hidden_layer = EUNN(hidden_size, capacity=capacity).terminate().to(device)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, output_size, bias=True)\n",
    "        self.modrelu = ModReLU(hidden_size).to(device)\n",
    "        \n",
    "        # initialize weights\n",
    "        v = np.sqrt(2)/np.sqrt(num_labels + num_inputs)\n",
    "        self.input_layer.bias.data[:] = 0.01\n",
    "        self.modrelu.bias.data[:] = 0.01\n",
    "        self.output_layer.bias.data[:] = 0.01\n",
    "        torch.nn.init.uniform_(self.input_layer.weight.data, -v, v)\n",
    "        torch.nn.init.uniform_(self.output_layer.weight.data, -v, v)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        ''' input.shape = (# timesteps, #batches, #input_size) '''\n",
    "        # apply input layer\n",
    "        input_shape = input.shape\n",
    "        reshaped_input = input.view(-1, input_shape[-1])\n",
    "        result = self.input_layer(reshaped_input)\n",
    "        input = self.input_layer(reshaped_input).view(*(input_shape[:-1] + (-1,)))\n",
    "        \n",
    "        # make input complex\n",
    "        input = torch.stack([input, torch.zeros_like(input)], -1)\n",
    "        \n",
    "        # create internal state\n",
    "        state = torch.zeros_like(input[0])\n",
    "        \n",
    "        # recurrent loop\n",
    "        for inp in input:\n",
    "            state = self.modrelu(inp + self.hidden_layer(state))\n",
    "        \n",
    "        # return\n",
    "        return self.output_layer(state[...,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "_Fna6vUT1qPw"
   },
   "source": [
    "```\n",
    "device = 'cuda'\n",
    "x = torch.randn(28*28, 100, 1, device='cuda')\n",
    "eurnn = EURNN(input_size=1, hidden_size=200, output_size=10, capacity=2).to('cuda')\n",
    "%time eurnn = eurnn.initialize()\n",
    "%time result = eurnn(x)\n",
    "result.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xGtaHSmu1qPy"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eCN1BR0z1qPz"
   },
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "num_labels = 10 # Number of different types of labels (1-10)\n",
    "width, height = 28, 28 # width / height of the image\n",
    "num_pixels = width*height\n",
    "\n",
    "# pixel permutation idxs\n",
    "perm_idxs = list(range(num_pixels))\n",
    "np.random.RandomState(seed=0).shuffle(perm_idxs)\n",
    "\n",
    "# reverse pixel permutation idxs\n",
    "rev_perm_idxs = [perm_idxs.index(i) for i in range(num_pixels)]\n",
    "\n",
    "# Training Parameters\n",
    "num_steps = 1000 # Number of training steps to run\n",
    "test_size = 10000 # Test data set size\n",
    "valid_size = 10000 # Validation data set size\n",
    "train_size = 60000 - valid_size # Size of the training set\n",
    "batch_size = 100 # Batch size\n",
    "test_batch_size = 200 # batch size for calculating the validation/test loss\n",
    "\n",
    "# RNN Parameters\n",
    "num_inputs = 1 # input dimension [1=pixel-by-pixel]\n",
    "num_steps_rnn = num_pixels // num_inputs # sequential dimensionality of rnn\n",
    "num_hidden_rnn = 256 # hidden layer dimension\n",
    "capacity_rnn = 2 # capacity of eunn\n",
    "\n",
    "# Optimization parameters\n",
    "learning_rate = 0.0001 # learning rate\n",
    "\n",
    "# cuda\n",
    "cuda = True\n",
    "device = torch.device('cuda' if cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f731214e-9fca-f54d-91be-60475207ba64",
    "colab_type": "text",
    "id": "A-ufYpDg1qP4"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8lzKnEOk1qP5"
   },
   "source": [
    "We use the custom MNIST data fetcher from `fetch_mnist.py`.\n",
    "\n",
    "The image values are specified by an integer between 0 and 255. We convert these pixel values to a float between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_-0DpJg1qP8"
   },
   "source": [
    "In contrast with the Convolutional Neural Networks, we do a pixel-by-pixel recognition of the digit image where the individial pixels are permuted with a fixed permutation defined by `perm_idx`. This fixed permutation is necessary for good performance of the RNN, as otherwise the the end of the pixel stream contains too many zeros for the RNN to retain its internal state. This is a good benchmark task for a recurrent neural network. The performance of this architecture will obviously be worse than for a convnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "55dd8ffd-6011-49a3-a1fe-c6933c4187b7",
    "_uuid": "840f7b1c60d1a2d5b2222a7c53b2b9d08aac9169",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ndzL_mOZ1qP9",
    "outputId": "591359c4-8506-4514-cdce-9ac977ea687b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:\ttorch.Size([784, 50000, 1])\n",
      "train labels shape:\ttorch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = fetch_mnist()\n",
    "data = np.vstack([train_data, test_data])\n",
    "np.random.shuffle(data)\n",
    "\n",
    "train_data = data[:-test_size-valid_size]\n",
    "valid_data = data[-test_size-valid_size:-test_size]\n",
    "test_data  = data[-test_size:]\n",
    "\n",
    "def get_values_labels(data):\n",
    "    labels = torch.tensor(data[:,0], dtype=torch.int64, device=device)\n",
    "    values = torch.tensor(data[:,1:][:,perm_idxs]/255, dtype=torch.float32, device=device).view(-1, num_steps_rnn, num_inputs)\n",
    "    return values, labels\n",
    "    \n",
    "train_values, train_labels = get_values_labels(train_data)\n",
    "valid_values, valid_labels = get_values_labels(valid_data)\n",
    "test_values, test_labels = get_values_labels(test_data)\n",
    "\n",
    "train_values = train_values.permute(1,0,2).contiguous()\n",
    "valid_values = valid_values.permute(1,0,2).contiguous()\n",
    "test_values = test_values.permute(1,0,2).contiguous()\n",
    "\n",
    "\n",
    "print(f'train data shape:\\t{train_values.shape}')\n",
    "print(f'train labels shape:\\t{train_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "URPYkn_U1qQG"
   },
   "source": [
    "We can visualize the different digits by writing a visualization function that reshapes the 784D train and test values into a 28x28 grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "lSzq5es31qQI",
    "outputId": "6fcdf4b7-f9b8-4bfd-be12-f0fd469442e8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_digit(digit_array):\n",
    "    plt.imshow(digit_array.cpu().numpy().reshape(num_pixels)[rev_perm_idxs].reshape(width, height), cmap='Greys')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "show_digit(train_values[:,31])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e33cb0c7-51ed-02ae-ebb2-dcda12832da6",
    "colab_type": "text",
    "id": "iexPGidk1qQQ"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions to save checkpoints during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename(step, accuracy, loss):\n",
    "    return os.path.join(CHECKPOINTFOLDER, f'step_{step}_acc={accuracy:.2f}_loss={loss:.2f}.pkl')\n",
    "    \n",
    "def parse_filename(filename):\n",
    "    pattern = re.compile(r'(?:step_)(\\d*)(?:_acc=)(\\d*\\.\\d*)(?:_loss=)(\\d*\\.\\d*)(?:.pkl)')\n",
    "    match = pattern.findall(filename)[0]\n",
    "    return int(match[0]), float(match[1]), float(match[2])\n",
    "\n",
    "def history():\n",
    "    all_filenames = sorted([parse_filename(fn) for fn in os.listdir(CHECKPOINTFOLDER)])\n",
    "    return zip(*all_filenames)\n",
    "\n",
    "def last_filename():\n",
    "    steps, accuracies, losses = history()\n",
    "    return filename(steps[-1], accuracies[-1], losses[-1])\n",
    "\n",
    "def best_filename():\n",
    "    steps, accuracies, losses = history()\n",
    "    best_accuracy = max(accuracies)\n",
    "    best_index = accuracies.index(best_accuracy)\n",
    "    return filename(steps[best_index], accuracies[best_index], losses[best_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rISrr9wW1qQS"
   },
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RPjMR9841qQT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(CHECKPOINTFOLDER):\n",
    "    os.mkdir(CHECKPOINTFOLDER)\n",
    "    \n",
    "start_step = -1\n",
    "model = EURNN(num_inputs, num_hidden_rnn, num_labels, capacity_rnn).to(device)#.initialize()\n",
    "if len(os.listdir(CHECKPOINTFOLDER))>0:\n",
    "    model.load_state_dict(torch.load(last_filename()))\n",
    "    start_step, _, _ = parse_filename(last_filename())\n",
    "    print('model loaded')\n",
    "start_step += 1\n",
    "\n",
    "print(start_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmDdc7pn1qQY"
   },
   "source": [
    "We use the categorical cross entropy loss for training the model.\n",
    "\n",
    "As optimizer we could use a Gradient Descent optimizer [with or without decaying learning rate] or one of the more sophisticated (and easier to optimize) optimizers like Adam or RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "7fbe419e-7ce2-4d72-bb31-8b27e8161f1b",
    "_uuid": "bb1b6d4fb5504400ed7678d8e95d0a4478b5f409",
    "colab": {},
    "colab_type": "code",
    "id": "pd_U4a-11qQZ"
   },
   "outputs": [],
   "source": [
    "# loss\n",
    "lossfunc = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# accuracy\n",
    "def accuracy(logits, labels):\n",
    "    return 100*np.mean(np.argmax(logits.data.cpu().numpy(), 1) == labels.data.cpu().numpy())\n",
    "\n",
    "# RMSprop Optimizer\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlTQOEQt1qQg"
   },
   "source": [
    "Start the training. This takes about 24 hours. [enable by converting the cell to a code cell]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "32786a5c-0388-412d-b6da-ee5ace604eda",
    "_uuid": "9c935ac4a1d1964b85513da422ebf60085dca0e3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "8y0D22Je1qQg",
    "outputId": "a22429a1-09a7-4565-d7ab-89dab9f76340"
   },
   "source": [
    "from tqdm import trange\n",
    "steps = trange(start_step, num_steps, 1)\n",
    "for step in steps:\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # initialize model\n",
    "    model.hidden_layer.initialize()\n",
    "    \n",
    "    if False:\n",
    "        with torch.no_grad():\n",
    "            if step%(train_size//batch_size) == 0 or step == num_steps - 1:\n",
    "                val_acc = np.zeros(valid_size//test_batch_size)\n",
    "                val_loss = np.zeros(valid_size//test_batch_size)\n",
    "                # we need to split the calculation of the validation loss in batches\n",
    "                # to avoid memory problems.\n",
    "                for i in range(0, valid_size, test_batch_size):\n",
    "                    valid_logits = model(valid_values[:,i:i+test_batch_size].contiguous())\n",
    "                    val_loss[i//test_batch_size] = lossfunc(valid_logits, valid_labels[i:i+test_batch_size]).item()\n",
    "                    val_acc[i//test_batch_size] = accuracy(valid_logits, valid_labels[i:i+test_batch_size]).item()\n",
    "                    del valid_logits\n",
    "\n",
    "                fn = filename(step, val_acc.mean(), val_loss.mean())\n",
    "                print(fn.split('/')[-1])\n",
    "                torch.save(model.state_dict(), fn)\n",
    "    \n",
    "    # train\n",
    "    idxs = np.random.randint(0, train_size, batch_size)\n",
    "    batch_values = train_values[:,idxs]\n",
    "    batch_labels = train_labels[idxs]\n",
    "    logits = model(batch_values)\n",
    "    loss = lossfunc(logits, batch_labels)\n",
    "    acc = accuracy(logits, batch_labels)\n",
    "    loss.backward()\n",
    "    steps.set_postfix(acc=f'{acc.item():.2f}', loss=f'{loss.item():.8f}')\n",
    "    optimizer.step()\n",
    "    del batch_values, batch_labels, logits, loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wgQXeqRv1qQm"
   },
   "source": [
    "Training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "E-2h4IdS1qQq",
    "outputId": "04cde66e-7043-4c7c-b0b2-3ac1563c668e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x158.4 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from mplstyle import paper\n",
    "steps, acc, loss = history()\n",
    "\n",
    "with custom_style():\n",
    "    fig, ax_loss = plt.subplots()\n",
    "    ax_acc = ax_loss.twinx()\n",
    "\n",
    "    plt.sca(ax_acc)\n",
    "    plt.plot(steps, acc, '-', color='C1')\n",
    "    plt.ylabel('Accuracy [\\%]', color='C1');\n",
    "    plt.tick_params('y', colors='C1')\n",
    "    plt.ylim(69,100)\n",
    "    plt.yticks([70,80,90,100])\n",
    "\n",
    "    plt.sca(ax_loss)\n",
    "    plt.plot(steps, loss, '-', color='C0')\n",
    "    plt.ylabel('Log Loss', color='C0');\n",
    "    plt.tick_params('y', colors='C0')\n",
    "    plt.ylim(-0.05,2)\n",
    "\n",
    "    plt.xlim(0, (max(steps)+100)//100*100)\n",
    "    plt.xlabel('Training Steps')\n",
    "    #plt.title('Validation Loss / Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjlx0q9u1qQw"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8QQX8VhZ1qQw"
   },
   "source": [
    "Finally, the accuracy on the test set can be evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBUNdqzR1qQx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 91.50\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_filename()))\n",
    "model.hidden_layer.initialize()\n",
    "with torch.no_grad():\n",
    "    # we need to split the calculation of the test loss in batches\n",
    "    # to avoid memory problems.\n",
    "    test_accuracy = np.zeros(test_size//test_batch_size)\n",
    "    for i in range(0, test_size, test_batch_size):\n",
    "        test_logits = model(test_values[:,i:i+test_batch_size].contiguous())\n",
    "        test_accuracy[i//test_batch_size] = accuracy(test_logits, test_labels[i:i+test_batch_size]).item()\n",
    "test_accuracy = test_accuracy.mean()\n",
    "print(f'Test Accuracy = {test_accuracy:5.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vAM5V9P1qQ3"
   },
   "source": [
    "Visualize an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SWoFQtqb1qQ4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction=4\ttarget=4\n"
     ]
    }
   ],
   "source": [
    "N = 318\n",
    "show_digit(test_values[:,N])\n",
    "test_logits = model(test_values[:,N:N+1])\n",
    "prediction = torch.argmax(test_logits[0]).item()\n",
    "target = test_labels[N].item()\n",
    "print(f'prediction={prediction}\\ttarget={target}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08_photontorch_eunn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
